\section{Usability Testing} \label{sc:UsabilityTesting}
Usability tests were used to help ensure an intuitive and simple user interface for the system. In these tests a test participant would solve a number of tasks designed to illuminate any problem areas in the UI design. The purpose of the tests was to identify any problems with the UI that might make it difficult or impossible to perform the tasks the system is supposed to support.
\par
The system was tested both from the perspective of an admin and an employee. This was done to cover the entire system, though the admin parts of the system were the main focus. 
\par
The difference between the two parts is that the employee part needs to be very intuitive, while this is less important for the admin part. The administrators of the system are expected to learn how to use the system beforehand, especially the tagging functionality, as this part of the system is advanced.
\par
The two parts were tested with different test participants, with the representatives from Aalborg Zoo testing the application as admins, since this is how they will work with it. The remaining test participants were students.
\par
Eight usability tests were conducted, with three of these being with admin access. The number of admin tests were judged to be sufficient as they were conducted with the people who are going to use the system as test participants, as well as an initial test mainly for practice purposes. 
\par
The number of employee tests were larger since they were quick to conduct, lasting less than 20 minutes each. This was mainly due to the more limited functionality that employees have access to. Because the time span of the employee tests were shorter, it was easy to conduct several of them in a relatively short period of time. 
\par
The tests were conducted at different stages of development. During the exploratory phase, there were no tests. Instead the wireframes and prototypes (see \autoref{ch:ui_design}) were evaluated through less structured talks with the client. Later, both assessment tests and validation tests were conducted on the admin part of the application, in order to refine the UI design. 
\par
Only validation tests were conducted on the employee part of the application, since this interface is relatively simple and there was not as much focus on this part. 
\par
The results of the tests influenced the final UI design as described in \autoref{ch:ui_design}.


\subsection{Preparations}
The tasks in the usability tests were designed to make the participants interact with the different parts of the system. It was, however, important that the tasks were not too leading, so the participants had the freedom to solve the tasks intuitively. 
\par
The two parts of the system (admin and employee) were tested separately, using different tasks. The employee tests were short, with only two tasks (see \autoref{app:usabilityTest}), while the admin test consisted of 10 tasks. The difference in number of tasks is due to the difference in complexity between the two parts. 
\par
An example task for the admin part of the application is the following:

\begin{quote}
\textit{You have received a new switch with the name "TL-10". The switch is produced by "TP-Link". It has the IP address "149.220.101.90‚Äù and a unique bar code "11S362-SKl19". Add the switch to the system, using this information.}
\end{quote}

The purpose of this task was to have the test participant add an asset to the system, in order to see if the procedure for doing this was intuitive and simple. Another purpose of the test was to see, if the participant performed the task in the intended way, for instance using fields to add the IP address to the switch asset.
\par 
In the formulation of the tasks it was attempted to avoid words used in the application, so the participant was not guided too much in their tasks. This has led to some strange formulations of tasks, but it was decided that the presence of the test moderator could alleviate any confusion.
\par
Other admin tasks included asking the test participant to create a tag and add it to an asset. Some changes have been made to the list of tasks over time, but an overview of the latest configuration of tasks can be seen in \autoref{app:usabilityTest}. 

\subsection{Setup}
The usability tests were conducted either in a meeting room at Aalborg Zoo, a group room, or a table in the hallway. It was not considered necessary for the project to conduct the tests in a lab, as the chosen environments were quiet enough as to not distract the test participants during the test.
\par
%Present at the tests were a test moderator and a logger. The test moderator conducted and controlled the test while the logger took notes.
%\par
The tests were not recorded, but instead the Instant Data Analysis (IDA) method was used. This method is much faster than the traditional video analysis method, as there is no video material to be transcribed and analysed. Instead the people present at the test were debriefed directly afterwards, noting down their findings. The IDA method may be less precise, as it relies on human memory and note taking rather than recordings. In this project the time advantage of the IDA method outweighed its less detailed results compared to the video analysis method. \citep{IDA}

\subsection{During the test}
The usability test started with a short introduction to the system, to make sure that the test participant knew what the test was about. The participant was also instructed in how the test would be conducted. During this instruction they were asked to express their thoughts about the system during the test. This could be any expectations they had for doing certain actions or when they were surprised or confused by something the system did. By doing so it became clear when a problem arose and sometimes how the problem might be solved. After the introduction the participant was given the prepared list of tasks.
\par
While the participant performed the tasks the test moderator sat beside them and would remind them to think aloud. It was important, however, that the test moderator did not interfere with the test unless the test participant got stuck on a task. In that case the moderator could step in and help.
\par
During the test a logger was in the room taking notes on anything the test participant said and did, that was relevant to the evaluation of the UI design. 
\par
After all the tasks were completed, the test participant was asked about their experience with the application. The moderator or logger might ask about instances where the test participant had trouble completing a task, or more generally what the test participant thought of their interactions with the system. This provided the opportunity to get a better understanding of what the test participant was thinking during the test, without distracting them from the tasks. It also gave the test participant the opportunity to make remarks on the system as a whole, including any suggestions they may have for improvements.

\subsection{Test Evaluation}
After the test and following talk with the test participant, the moderator, logger, and any others that might have been present during the test, talked about the results, noting down anything they noticed during the test. This increased the chances that all issues with the UI design were caught and noted down despite not having any video recordings of the test. 
\par
This discussion resulted in a ranked list of usability problems that should be solved in the design of the application. These problems were ranked based on their severity, as described in \autoref{tab:ProblemSeverity}.

\begin{table}[H]
    \centering
    \begin{tabular}{|c||c|c|c|}
        \hline
        & Delay & Irritation & Expectation vs. actual
        \\
        \hline
        \hline
        Cosmetic & < 1 minute & Low & Small diff.
        \\
        \hline
        Serious & Several minutes & Medium & Significant diff.
        \\
        \hline
        Critical & Total (user stops) & High & Critical diff.
        \\
        \hline
    \end{tabular}
    \caption{Table for finding the different severity ranks of a usability problem. Based on Jakob Nielsen's rating scheme \citep[Table 8]{UsabilityEngineering}, modified through the DEB course.}
    \label{tab:ProblemSeverity}
\end{table}

% As seen in \autoref{tab:ProblemSeverity} a problem can be either cosmetic, serious or critical. The table is a guide to categorize a problem as either of these severity ranks. As an example, a problem that caused a delay of several minutes when performing a task might be categorized as serious. The irritation a problem causes the user is also relevant when ranking the problem, as is the difference between what the user expects to happen and what actually happens when performing an action. 
% \par

\newpage

Using guidelines from \autoref{tab:ProblemSeverity} the problems uncovered during the usability tests were ranked. The list of problems from the tests with the clients as well as the employee tests can be seen in \autoref{tab:AdminProblems} and \autoref{tab:EmployeeProblems}. 
\par
\autoref{tab:AdminProblems} contains the problems discovered during the admin tests, while \autoref{tab:EmployeeProblems} shows results from the employee tests. Both tables contain a description and a severity rating for each problem. 
\par
The problems in the tables each have an unique index. This index consists of a letter signifying which test participant was doing the test, and a number counting up for every problem in that test. Each problem is also marked with which task it occurred during and a severity rating. The tasks referred to can be seen in \autoref{app:usabilityTest}. The severity of a problem is categorized as either \textit{critical}, \textit{serious} or \textit{cosmetic}, as described in \autoref{sc:UsabilityTesting}. 
\par
The first usability test was conducted with a student as test participant. This test had the main purpose of practicing how do conduct a usability test. As a result, the problems found have not been recorded in the following list. The admin test problems are thus only from tests with the zoo representatives, while the employee tests were all conducted with student.

\begin{longtable}[c]{| c | c | p{8cm} | c |}
    \hline
    \textbf{Index} & \textbf{Task} & \textbf{Description} & \textbf{Severity}
    \\
    \hline
    A-1 & 1 & The test participant had problems understanding that the tags could be used to group assets. The test participant solved the task, but not in the intended way. &  Cosmetic
    \\
    \hline
    A-2 & 1 & The test participant did not understand that the tags could contain tags, and that these would be carried over to the connected assets. &  Cosmetic
    \\
    \hline
    A-3 & 2 & The test participant inserted the information given into the description, but quickly realized that they could use the fields. &  Cosmetic
    \\
    \hline
    A-4 & 3 & The test participant did not find changing the fields on the tag intuitive. Instead they went directly to the asset and added the number of ports to it directly. &  Cosmetic
    \\
    \hline
    A-5 & 6 & The test participant went to the specific asset to delete it, instead of doing it from the list. This was not possible and they had trouble solving the task. &  Serious
    \\
    \hline
    B-1 & 2 & The test participant got confused about the tags and fields in on the asset and wrote all information in the description. &  Cosmetic
    \\
    \hline
    B-2 & - & The test participant expected to be able to select a tag from the suggested list when searching. This functionality was not working. & Cosmetic
    \\
    \hline
    B-3 & 2 & The test participant got a little confused when the suggestions did not appear as the search field was selected. & Cosmetic
    \\
    \hline
    B-4 & 2 & The test participant wondered what the pencil button for editing a field meant. After an explanation this made sense after all. & Cosmetic
    \\
    \hline
    B-5 & 2 & In asset editor when trying to add a tag, the test participant could not exit the parent tag. & Cosmetic
    \\
    \hline
    \caption{Table of problems found during usability tests of the system when user had admin access}
    \label{tab:AdminProblems}
\end{longtable}

\begin{longtable}{| c | c | p{8cm} | c |}
        \hline
        \textbf{Index} & \textbf{Task} & \textbf{Description} & \textbf{Severity}
        \\
        \hline
        C-1 & 1 & It was difficult to find the asset page. The test participant did not know what an asset was and did not associate a computer with an asset. They tried looking in Setting and Help. Test moderator then explained what an asset was, after which the test participant went to the asset page. & Critical
        \\
        \hline
        C-2 & 2 & Test participant expects the return button to sent the comment. They quickly press Send button after this. & Cosmetic
        \\
        \hline
        D-1 & 1 & The test participant was confused about what an asset was. They try looking in Settings and Help. They get stuck on the Home page and think the comments are assets. They open these before realizing they are not asset and then going to the asset page. & Serious
        \\
        \hline
        D-2 & 1 & The test participant was surprised then a lot of assets appeared after changing department. It was not obvious that changing departments had and effect in the application & Cosmetic
        \\
        \hline
        E-1 & 1 & The test participant quickly figures out how to change department, but when nothing happened on the Home page, they changed the department back again because they thought they made a mistake. They then looking in Settings and Help & Serious
        \\
        \hline
        E-2 & 1 & The test participant thinks comments on the Home page are assets. They open each of then in turn & Serious
        \\
        \hline
        E-3 & 1 & The test participant does not use the search field, but instead looks through the list for the asset &  Cosmetic
        \\
        \hline
        E-4 & 2 & There were problems with the task description. The test participant thought they had to find a "direct connection" to the admin and starts looking for this elsewhere in the application & Serious
        \\
        \hline
        F-1 & 1 & The test participant does not recognize that they are in the wrong department. They use the search field on the asset page to try and find the computer in the wrong department. They get stuck and try looking in Settings and Help. Moderator explains about departments but they are still stuck and try finding the department through tags in the search field. Moderator explains about the search field and test participant then quickly find and changes departments. It was not obvious enough that the department menu button was a button. & Critical
        \\
        \hline
        F-2 & 1 & The test participant tries to find the right department through tags on the asset page. They get stuck and can not exit tag mode again. &  Critical
        \\
        \hline
        F-3 & 1 & Test participant does not use the search field but looks for the asset in the list instead. & Cosmetic
        \\
        \hline
        F-4 & 2 & The test participant tries to press the return button to send the comment. They then press the Send button. & Cosmetic
        \\
        \hline
        G-1 & 1 & The test participant tries looking in Settings and Help first. They find and change the department, sees changes in the comments on the Home page but changes department back. Then they went to the asset page but in the wrong department. They try to search for the computer in the wrong department, before realizing they changed the department back and then changes it to the right one and quickly finds the asset. & Serious
        \\
        \hline
        
    \caption{Table of problems found during usability tests of the system when user did not have admin access}
    \label{tab:EmployeeProblems}
   
\end{longtable}

To give an understanding of the type of problems found, an example will be given. Most of the problems for both tests with the representatives from Aalborg zoo were cosmetic. However, one was serious (Index A-5 in \autoref{tab:AdminProblems}). Here the test participant was asked to delete an asset from the system and went to that asset's page to do so. This was, however, not possible at the time, as there was no button for deleting the asset from this page. The test participants expectations were therefore not met and they were stuck for several minutes not realizing that they were supposed to delete the asset from the asset list page. This problem led to the addition of a delete button on the page of each specific asset.